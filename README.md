# Face-Monitor

The Face Monitoring System integrates advanced computer vision techniques and machine learning models to interpret and analyze facial expressions and gestures in real-time. Built upon the Mediapipe framework and employing a Gradient Boosting Classifier, the system captures intricate details such as facial landmarks, hand movements, and body poses from live video feeds. These features are meticulously processed to deduce the user's emotional state and level of engagement, delivering instantaneous feedback through an intuitive graphical interface.

The system operates by first detecting and tracking facial landmarks, which include points on the face like eyes, nose, and mouth, using Mediapipe's holistic approach. Concurrently, it tracks hand gestures and body movements, providing a comprehensive view of the user's actions and expressions. This data is then fed into a Gradient Boosting Classifier trained to classify emotional states such as distraction, focus, and drowsiness based on patterns derived from these features. The classifier's predictions are continuously updated and displayed in real-time on the user interface, providing immediate feedback and insights.

Validation of the system's performance involves rigorous testing using established metrics such as accuracy, precision, recall, and F1-score. These metrics quantify the system's ability to correctly classify and interpret user behavior, ensuring reliable operation across various scenarios and environmental conditions. By empowering users with actionable insights into their emotional and engagement states, the Face Monitoring System aims to enhance productivity, promote self-awareness, and facilitate improved human-computer interaction in interactive and monitoring applications.

This project represents a significant advancement in the field of human-computer interaction, leveraging state-of-the-art technologies to offer real-time emotional and engagement monitoring capabilities. It sets a benchmark for future developments in behavior analysis systems, paving the way for applications in fields such as education, healthcare, and beyond, where understanding human behavior in real-time can lead to improved outcomes and experiences.

## Home Page
![Screenshot 2024-06-30 135405](https://github.com/user-attachments/assets/3c5a4631-b30f-4ec0-a129-36a4d7f4757a)

## Model Execution 
![image](https://github.com/user-attachments/assets/db7ea40a-5b9e-4893-a096-06ceaee6c9a8)

## Result Page 
![Screenshot 2024-07-01 201447](https://github.com/user-attachments/assets/0148ec98-641e-4603-a43e-32d6c551d18a)

